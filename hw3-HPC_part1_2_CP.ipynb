{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "EHwuDPOMgjze"
   },
   "source": [
    "---   \n",
    "# HW3 - Transfer learning\n",
    "\n",
    "#### Due October 30, 2019\n",
    "\n",
    "In this assignment you will learn about transfer learning. This technique is perhaps one of the most important techniques for industry. When a problem you want to solve does not have enough data, we use a different (larger) dataset to learn representations which can help us solve our task using the smaller task.\n",
    "\n",
    "The general steps to transfer learning are as follows:\n",
    "\n",
    "1. Find a huge dataset with similar characteristics to the problem you are interested in.\n",
    "2. Choose a model powerful enough to extract meaningful representations from the huge dataset.\n",
    "3. Train this model on the huge dataset.\n",
    "4. Use this model to train on the smaller dataset.\n",
    "\n",
    "\n",
    "### This homework has the following sections:\n",
    "1. Question 1: MNIST fine-tuning (Parts A, B, C, D).\n",
    "2. Question 2: Pretrain on Wikitext2 (Part A, B, C, D)\n",
    "3. Question 3: Finetune on MNLI (Part A, B, C, D)\n",
    "4. Question 4: Finetune using pretrained BERT (Part A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "CYghzuesgjzh"
   },
   "source": [
    "---   \n",
    "## Question 1 (MNIST transfer learning)\n",
    "To grasp the high-level approach to transfer learning, let's first do a simple example using computer vision. \n",
    "\n",
    "The torchvision library has pretrained models (resnets, vggnets, etc) on the Imagenet dataset. Imagenet is a dataset\n",
    "with 1.3 million images covering over 1000 classes of objects. When you use one of these models, the weights of the model initialize\n",
    "with the weights saved from training on imagenet.\n",
    "\n",
    "In this task we will:\n",
    "1. Choose a pretrained model.\n",
    "2. Freeze the model so that the weights don't change.\n",
    "3. Fine-tune on a few labels of MNIST.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "t0tWPIOvgjzi"
   },
   "source": [
    "#### Choose a model\n",
    "Here we pick any of the models from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "QpfP9IVGi-wN",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "VWGpkMoDgjzi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# init the pretrained feature extractor\n",
    "pretrained_resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = pretrained_resnet18.fc.in_features\n",
    "pretrained_resnet18.fc = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1571675925898,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "KLdJYN6LjKG-",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1f6ad9e9-d204-47b8-f450-1a5b73d81d05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ftrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "JMCEU7xJgjzl"
   },
   "source": [
    "#### Freeze the model\n",
    "Here we freeze the weights of the model. Freezing means the gradients will not backpropagate\n",
    "into these weights.\n",
    "\n",
    "By doing this you can think about the model as a feature extractor. This feature extractor outputs\n",
    "a **representation** of an input. This representation is a matrix that encodes information about the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "VSrOhLuxgjzm"
   },
   "outputs": [],
   "source": [
    "def freeze_model(model): #feature extract\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfreeze_model(model): #finetune \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "#freeze_model(pretrained_resnet50)\n",
    "freeze_model(pretrained_resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "FeGPuYysgjzp"
   },
   "source": [
    "#### Init target dataset\n",
    "Here we define the dataset we are actually interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 17034,
     "status": "ok",
     "timestamp": 1571675965066,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "4U8GEEXPgjzq",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "15da41c5-3380-4156-98df-85946bdf8263"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import  MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  train/val  split\n",
    "mnist_dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_train, mnist_val = random_split(mnist_dataset, [55000, 5000])\n",
    "\n",
    "mnist_train = DataLoader(mnist_train, batch_size=32)\n",
    "mnist_val = DataLoader(mnist_val, batch_size=32)\n",
    "\n",
    "# test split\n",
    "mnist_test = DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "zLFVP4KWXZR8"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1571675972630,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "hq1dF__MPnCH",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "aba6eadc-8be3-481b-812e-938de009567e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(mnist_test):\n",
    "  print(len(batch))\n",
    "  print(batch[0].shape)\n",
    "  \n",
    "  a = np.repeat(batch[0], 3, axis=1)\n",
    "  print(a.shape)\n",
    "  #print(batch[1].shape)\n",
    "  #print(batch[1])\n",
    "  break\n",
    "#mnist_test batch has length of 2\n",
    "#batch[0] is test_X data : [32, 1, 28, 28] => 32 images in each batch and each image has a dimension of 28 x 28 pixels.\n",
    "#batch[1] is test_Y data: [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1571675974669,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "OOVeF9b_BEDj",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "3411e530-15e1-4c9d-bf98-37e24e9cc403"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1719"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Q-e3vIrYgjzs"
   },
   "source": [
    "### Part A (init fine-tune model)\n",
    "decide what model to use for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "MEYxfsvMgjzt"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#num_ftrs is 512\n",
    "#hidden_dim = 100 #arbitrary\n",
    "\n",
    "\n",
    "def init_fine_tune_model(num_ftrs):\n",
    " \n",
    "    num_classes = 10\n",
    "    \n",
    "    fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, num_classes))\n",
    "\n",
    "    return fc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "3Em-54qIgjzv"
   },
   "source": [
    "### Part B (Fine-tune (Frozen))\n",
    "\n",
    "The actual problem we care about solving likely has a different number of classes or is a different task altogether. Fine-tuning is the process of using the extracted representations (features) to solve this downstream task  (the task you're interested in).\n",
    "\n",
    "To illustrate this, we'll use our pretrained model (on Imagenet), to solve the MNIST classification task.\n",
    "\n",
    "There are two types of finetuning. \n",
    "\n",
    "#### 1. Frozen feature_extractor\n",
    "In the first type we pretrain with the FROZEN feature_extractor and NEVER unfreeze it during finetuning.\n",
    "\n",
    "\n",
    "#### 2. Unfrozen feature_extractor\n",
    "In the second, we finetune with a FROZEN feature_extractor for a few epochs, then unfreeze the feature extractor and finish training.\n",
    "\n",
    "\n",
    "In this part we will use the first version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "2AsIanR_7DYR"
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for i, (images, labels) in enumerate(loader):\n",
    "          images = np.repeat(images, 3, axis=1) #convert to 3 channel\n",
    "          inputs, labels = images.to(device), labels.to(device)    \n",
    "          outputs = F.softmax(model(inputs), dim=1)\n",
    "          predicted = outputs.max(1, keepdim=True)[1]\n",
    "          total += labels.size(0)\n",
    "          correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        \n",
    "    return (100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "e-PdDV8Cgjzv"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "def FROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, num_epochs, mnist_train, mnist_val):\n",
    "    \"\"\"\n",
    "    model is a feature extractor (resnet).\n",
    "    Create a new model which uses those features to finetune on MNIST\n",
    "    \n",
    "    return the fine_tune model\n",
    "    \"\"\"     \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    #num_epochs = 3\n",
    "    # INSERT YOUR CODE: (train the fine_tune model using features extracted by feature_extractor)\n",
    "    #first freeze the layers\n",
    "    freeze_model(feature_extractor)\n",
    "    \n",
    "    #create the finetune model\n",
    "    feature_extractor.fc = fine_tune_model #this is MLP toplayer\n",
    "    model = feature_extractor\n",
    "    \n",
    "    #create loss etc. \n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accs= []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs): \n",
    "      #train\n",
    "       \n",
    "      for i , (images, labels) in enumerate(mnist_train):\n",
    "            images = np.repeat(images, 3, axis=1) #convert to 3 channel\n",
    "            inputs, labels = images.to(device), labels.to(device)\n",
    "            #print(inputs.shape)\n",
    "            #inputs = inputs.unsqueeze_(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 500== 0:\n",
    "                val_acc = test_model(mnist_val, model)\n",
    "                val_accs.append(val_accs)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train Loss {}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(mnist_train), loss,  val_acc))\n",
    "                model.train() #go back to training\n",
    "  \n",
    "    return model, train_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "fJSv5FIfgjzy"
   },
   "source": [
    "### Part C (compute test accuracy)\n",
    "Compute the test accuracy of fine-tuned model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "_IgxtV8Bgjzy"
   },
   "outputs": [],
   "source": [
    "#def calculate_mnist_test_accuracy(feature_extractor, fine_tune_model, mnist_test):\n",
    "def calculate_mnist_test_accuracy(model, mnist_test):   \n",
    "    test_accuracy = test_model(mnist_test, model)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "AF6QjTn-gjz0"
   },
   "source": [
    "### Grade!\n",
    "Let's see how you did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 343831,
     "status": "ok",
     "timestamp": 1571677758971,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "nL4njzEfgjz1",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7506edeb-a762-40b4-ccdb-5cc4a2656702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [501/1719], Train Loss 2.0012693405151367, Validation Acc: 39.52\n",
      "Epoch: [1/10], Step: [1001/1719], Train Loss 1.5530532598495483, Validation Acc: 55.0\n",
      "Epoch: [1/10], Step: [1501/1719], Train Loss 1.3267741203308105, Validation Acc: 59.6\n",
      "Epoch: [2/10], Step: [501/1719], Train Loss 1.3506823778152466, Validation Acc: 64.14\n",
      "Epoch: [2/10], Step: [1001/1719], Train Loss 1.043002724647522, Validation Acc: 66.28\n",
      "Epoch: [2/10], Step: [1501/1719], Train Loss 1.0089775323867798, Validation Acc: 66.72\n",
      "Epoch: [3/10], Step: [501/1719], Train Loss 1.1376147270202637, Validation Acc: 68.9\n",
      "Epoch: [3/10], Step: [1001/1719], Train Loss 0.873719334602356, Validation Acc: 70.0\n",
      "Epoch: [3/10], Step: [1501/1719], Train Loss 0.8653533458709717, Validation Acc: 69.9\n",
      "Epoch: [4/10], Step: [501/1719], Train Loss 1.0356626510620117, Validation Acc: 71.1\n",
      "Epoch: [4/10], Step: [1001/1719], Train Loss 0.7839066386222839, Validation Acc: 71.98\n",
      "Epoch: [4/10], Step: [1501/1719], Train Loss 0.7843267917633057, Validation Acc: 71.42\n",
      "Epoch: [5/10], Step: [501/1719], Train Loss 0.972897469997406, Validation Acc: 72.34\n",
      "Epoch: [5/10], Step: [1001/1719], Train Loss 0.7266281843185425, Validation Acc: 72.88\n",
      "Epoch: [5/10], Step: [1501/1719], Train Loss 0.7315995097160339, Validation Acc: 72.66\n",
      "Epoch: [6/10], Step: [501/1719], Train Loss 0.9276345372200012, Validation Acc: 73.38\n",
      "Epoch: [6/10], Step: [1001/1719], Train Loss 0.6869994401931763, Validation Acc: 73.6\n",
      "Epoch: [6/10], Step: [1501/1719], Train Loss 0.6943244934082031, Validation Acc: 73.52\n",
      "Epoch: [7/10], Step: [501/1719], Train Loss 0.8924758434295654, Validation Acc: 74.12\n",
      "Epoch: [7/10], Step: [1001/1719], Train Loss 0.6585289835929871, Validation Acc: 74.32\n",
      "Epoch: [7/10], Step: [1501/1719], Train Loss 0.6670406460762024, Validation Acc: 74.1\n",
      "Epoch: [8/10], Step: [501/1719], Train Loss 0.864335298538208, Validation Acc: 74.92\n",
      "Epoch: [8/10], Step: [1001/1719], Train Loss 0.6372556686401367, Validation Acc: 74.64\n",
      "Epoch: [8/10], Step: [1501/1719], Train Loss 0.6464942693710327, Validation Acc: 74.62\n",
      "Epoch: [9/10], Step: [501/1719], Train Loss 0.8416225910186768, Validation Acc: 75.26\n",
      "Epoch: [9/10], Step: [1001/1719], Train Loss 0.6209186911582947, Validation Acc: 75.24\n",
      "Epoch: [9/10], Step: [1501/1719], Train Loss 0.6302366852760315, Validation Acc: 74.76\n",
      "Epoch: [10/10], Step: [501/1719], Train Loss 0.8227741718292236, Validation Acc: 75.5\n",
      "Epoch: [10/10], Step: [1001/1719], Train Loss 0.6080595254898071, Validation Acc: 75.42\n",
      "Epoch: [10/10], Step: [1501/1719], Train Loss 0.6173096299171448, Validation Acc: 75.2\n"
     ]
    }
   ],
   "source": [
    "def grade_mnist_frozen():\n",
    "    \n",
    "    # init a ft model\n",
    "    fine_tune_model = init_fine_tune_model(num_ftrs)\n",
    "    \n",
    "    # run the transfer learning routine\n",
    "    num_epochs = 10\n",
    "    model, train_losses, val_accs=FROZEN_fine_tune_mnist(pretrained_resnet18, fine_tune_model, num_epochs, mnist_train, mnist_val)\n",
    "    \n",
    "    # calculate test accuracy\n",
    "    test_accuracy = calculate_mnist_test_accuracy(model, mnist_test)\n",
    "    \n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_accuracy > 0.0, 'your accuracy is too low...'\n",
    "    \n",
    "    #save model\n",
    "\n",
    "    PATH_TO_FOLDER=  '/scratch/cp2530/myjupyter/'\n",
    "    torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'loss': train_losses[-1],\n",
    "            'frozen_test_accuracy': test_accuracy \n",
    "           \n",
    "            }, PATH_TO_FOLDER + \"models/ResNet18Freeze_CP\")\n",
    "\n",
    "    \n",
    "    \n",
    "    return test_accuracy\n",
    "    \n",
    "frozen_test_accuracy = grade_mnist_frozen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1571677763506,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "iHCeMzriZosm",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6f3f7b67-32ed-41a1-8442-4f4444497bc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_test_accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1571677770337,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "dXonYME5S4AD",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fae68da9-c0cd-41be-f2a6-7e6f0185425e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "QVwV_IIcgjz4"
   },
   "source": [
    "### Part D (Fine-tune Unfrozen)\n",
    "Now we'll learn how to train using the \"unfrozen\" approach.\n",
    "\n",
    "In this approach we'll:\n",
    "1. keep the feature_extract frozen for a few epochs (10)\n",
    "2. Unfreeze it.\n",
    "3. Finish training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "svL1x_xGgjz4"
   },
   "outputs": [],
   "source": [
    "def UNFROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, num_epochs, mnist_train, mnist_val):\n",
    "    \"\"\"\n",
    "    model is a feature extractor (resnet).\n",
    "    Create a new model which uses those features to finetune on MNIST\n",
    "    \n",
    "    return the fine_tune model\n",
    "    \"\"\"     \n",
    "    \n",
    "    # INSERT YOUR CODE:\n",
    "    # keep frozen for 10 epochs, let's do 5 frozen 5 unfrozen\n",
    "    # ... train\n",
    "    # unfreeze\n",
    "    # train for rest of the time\n",
    "    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    num_freeze_epochs = 5\n",
    "    # INSERT YOUR CODE: (train the fine_tune model using features extracted by feature_extractor)\n",
    "    #first freeze the layers\n",
    "    freeze_model(feature_extractor)\n",
    "    \n",
    "    #create the finetune model\n",
    "    feature_extractor.fc = fine_tune_model #this is MLP toplayer\n",
    "    model = feature_extractor\n",
    "    \n",
    "    #create loss etc. \n",
    "    param_list = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(\"num param req grad {}\".format(len(param_list)))\n",
    "    optimizer = optim.Adam(param_list, lr=2e-05, eps=1e-08)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accs= []\n",
    "    model.train()\n",
    "    for epoch in range(num_freeze_epochs): \n",
    "      #train\n",
    "      for i , (images, labels) in enumerate(mnist_train):\n",
    "            images = np.repeat(images, 3, axis=1) #convert to 3 channel\n",
    "            inputs, labels = images.to(device), labels.to(device)\n",
    "            #print(inputs.shape)\n",
    "            #inputs = inputs.unsqueeze_(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 500== 0:\n",
    "                val_acc = test_model(mnist_val, model) #calls model.eval()\n",
    "                val_accs.append(val_accs)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train Loss {}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_freeze_epochs, i+1, len(mnist_train), loss,  val_acc))\n",
    "                model.train() #go back to training\n",
    "    #do the unfreeze part          \n",
    "    num_left = abs(num_epochs - num_freeze_epochs)\n",
    "    unfreeze_model(feature_extractor) #hope this works\n",
    "    param_list = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    print('unfreeze')\n",
    "    print(\"num param req grad {}\".format(len(param_list)))\n",
    "    optimizer = optim.Adam(param_list, lr=2e-05, eps=1e-08)\n",
    "    \n",
    "    for epoch in range(num_left):\n",
    "      for i , (images, labels) in enumerate(mnist_train):\n",
    "          images = np.repeat(images, 3, axis=1) #convert to 3 channel\n",
    "          inputs, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          train_losses.append(loss.item())\n",
    "\n",
    "          # validate every 100 iterations\n",
    "          if i > 0 and i % 500== 0:\n",
    "              val_acc = test_model(mnist_val, model) #calls model.eval()\n",
    "              val_accs.append(val_accs)\n",
    "              print('Epoch: [{}/{}], Step: [{}/{}], Train Loss {}, Validation Acc: {}'.format( \n",
    "                         epoch+1, num_left, i+1, len(mnist_train), loss,  val_acc))\n",
    "              model.train()#go back to training\n",
    "           \n",
    "  \n",
    "    return model, train_losses, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "tup6jmqSgjz7"
   },
   "source": [
    "### Grade UNFROZEN\n",
    "Let's see if there's a difference in accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 1162217,
     "status": "ok",
     "timestamp": 1571686143724,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "Cx7o31Rfgjz7",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "345a09b2-b856-4766-cc58-501afc263db7"
   },
   "outputs": [],
   "source": [
    "def grade_mnist_unfrozen():\n",
    "    \n",
    "    # init a ft model\n",
    "    fine_tune_model = init_fine_tune_model(num_ftrs)\n",
    "    \n",
    "    # run the transfer learning routine\n",
    "    num_epochs = 10\n",
    "    model, train_losses, val_accs = UNFROZEN_fine_tune_mnist(pretrained_resnet18, fine_tune_model, num_epochs, mnist_train, mnist_val)\n",
    "    \n",
    "    # calculate test accuracy\n",
    "    test_accuracy = calculate_mnist_test_accuracy(model, mnist_test)\n",
    "    print(test_accuracy)\n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_accuracy > 0.0, 'your accuracy is too low...'\n",
    "    \n",
    "    #save model\n",
    "\n",
    "    PATH_TO_FOLDER =  '/scratch/cp2530/myjupyter/'\n",
    "    torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'loss': train_losses[-1],\n",
    "            'frozen_test_accuracy': test_accuracy \n",
    "           \n",
    "            }, PATH_TO_FOLDER + \"models/ResNet18UnFreeze_CP\")\n",
    "\n",
    "    \n",
    "    return test_accuracy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load traned unfrozen - did not work\n",
    "# feature_extractor = pretrained_resnet18\n",
    "# feature_extractor.fc = init_fine_tune_model(num_ftrs= 512)\n",
    "# model_unfrozen = feature_extractor\n",
    "\n",
    "# checkpoint = torch.load('/scratch/cp2530/myjupyter/models/ResNet18UnFreeze_CP')\n",
    "# model_unfrozen.load_state_dict(checkpoint['model_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# unfrozen_test_accuracy = calculate_mnist_test_accuracy(model_unfrozen, mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1571686177424,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "K8aEn0SPy776",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "d2895660-ad16-446b-dffc-915a63d4967b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num param req grad 4\n",
      "Epoch: [1/5], Step: [501/1719], Train Loss 2.0520548820495605, Validation Acc: 41.48\n",
      "Epoch: [1/5], Step: [1001/1719], Train Loss 1.665267825126648, Validation Acc: 54.38\n",
      "Epoch: [1/5], Step: [1501/1719], Train Loss 1.4178346395492554, Validation Acc: 59.5\n",
      "Epoch: [2/5], Step: [501/1719], Train Loss 1.3698444366455078, Validation Acc: 63.4\n",
      "Epoch: [2/5], Step: [1001/1719], Train Loss 1.1348119974136353, Validation Acc: 65.46\n",
      "Epoch: [2/5], Step: [1501/1719], Train Loss 1.0798262357711792, Validation Acc: 65.9\n",
      "Epoch: [3/5], Step: [501/1719], Train Loss 1.1365126371383667, Validation Acc: 68.48\n",
      "Epoch: [3/5], Step: [1001/1719], Train Loss 0.9355572462081909, Validation Acc: 69.38\n",
      "Epoch: [3/5], Step: [1501/1719], Train Loss 0.9325653910636902, Validation Acc: 69.18\n",
      "Epoch: [4/5], Step: [501/1719], Train Loss 1.0224225521087646, Validation Acc: 71.0\n",
      "Epoch: [4/5], Step: [1001/1719], Train Loss 0.8333955407142639, Validation Acc: 70.96\n",
      "Epoch: [4/5], Step: [1501/1719], Train Loss 0.8423619866371155, Validation Acc: 70.94\n",
      "Epoch: [5/5], Step: [501/1719], Train Loss 0.9561338424682617, Validation Acc: 72.3\n",
      "Epoch: [5/5], Step: [1001/1719], Train Loss 0.7670699954032898, Validation Acc: 72.44\n",
      "Epoch: [5/5], Step: [1501/1719], Train Loss 0.7857261300086975, Validation Acc: 72.5\n",
      "unfreeze\n",
      "num param req grad 64\n",
      "Epoch: [1/5], Step: [501/1719], Train Loss 0.16050481796264648, Validation Acc: 95.2\n",
      "Epoch: [1/5], Step: [1001/1719], Train Loss 0.02805282175540924, Validation Acc: 97.1\n",
      "Epoch: [1/5], Step: [1501/1719], Train Loss 0.10076822340488434, Validation Acc: 97.92\n",
      "Epoch: [2/5], Step: [501/1719], Train Loss 0.07858099043369293, Validation Acc: 98.14\n",
      "Epoch: [2/5], Step: [1001/1719], Train Loss 0.008136197924613953, Validation Acc: 98.3\n",
      "Epoch: [2/5], Step: [1501/1719], Train Loss 0.032767534255981445, Validation Acc: 98.3\n",
      "Epoch: [3/5], Step: [501/1719], Train Loss 0.011533111333847046, Validation Acc: 98.28\n",
      "Epoch: [3/5], Step: [1001/1719], Train Loss 0.0014768391847610474, Validation Acc: 98.44\n",
      "Epoch: [3/5], Step: [1501/1719], Train Loss 0.007814943790435791, Validation Acc: 98.52\n",
      "Epoch: [4/5], Step: [501/1719], Train Loss 0.002511844038963318, Validation Acc: 98.34\n",
      "Epoch: [4/5], Step: [1001/1719], Train Loss 0.0003813505172729492, Validation Acc: 98.56\n",
      "Epoch: [4/5], Step: [1501/1719], Train Loss 0.0025372207164764404, Validation Acc: 98.42\n",
      "Epoch: [5/5], Step: [501/1719], Train Loss 0.00135725736618042, Validation Acc: 98.66\n",
      "Epoch: [5/5], Step: [1001/1719], Train Loss 0.0010606348514556885, Validation Acc: 98.46\n",
      "Epoch: [5/5], Step: [1501/1719], Train Loss 0.003571033477783203, Validation Acc: 98.62\n",
      "98.55\n"
     ]
    }
   ],
   "source": [
    "unfrozen_test_accuracy = grade_mnist_unfrozen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "wowRq-bjgjz9"
   },
   "outputs": [],
   "source": [
    "assert unfrozen_test_accuracy > frozen_test_accuracy, 'the unfrozen model should be better'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L2AQiquLgjz_"
   },
   "source": [
    "--- \n",
    "# Question 2 (train a model on Wikitext-2)\n",
    "\n",
    "Here we'll apply what we just learned to NLP. In this section we'll make our own feature extractor and pretrain it on Wikitext-2.\n",
    "\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\n",
    "\n",
    "#### Part A\n",
    "In this section you need to generate the training, validation and test split. Feel free to use code from your previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5576,
     "status": "ok",
     "timestamp": 1571686270342,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "7mVHRPzmzQiE",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "581c73fc-59d3-4dc1-f2dd-61279ed578c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from jsonlines) (1.12.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-1.2.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "o8uadTxJzO8m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "NeRCn9FwcZrV"
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "PATH_TO_FOLDER=  '/scratch/cp2530/myjupyter/'\n",
    "\n",
    "def module_from_file(module_name, file_path):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "model_CP = module_from_file(\"model_CP\", PATH_TO_FOLDER+\"hw2/model_CP.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "Cq5P-ckagj0A"
   },
   "outputs": [],
   "source": [
    "\n",
    "#support code from lab\n",
    "from torchtext.datasets import WikiText2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import  MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "import numpy\n",
    "\n",
    "\n",
    "\n",
    "import io\n",
    "# def load_vectors(fname):\n",
    "#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, fin.readline().split())\n",
    "#     embedding_size = 300\n",
    "#     max_vocab_size = 35000\n",
    "#     embedding_dict = np.random.randn(max_vocab_size+2, embedding_size)\n",
    "#     all_train_tokens = []\n",
    "#     i = 0\n",
    "    \n",
    "#     for line in fin:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         all_train_tokens.append(tokens[0])\n",
    "#         embedding_dict[i+2] = list(map(float, tokens[1:]))\n",
    "#         i += 1\n",
    "#         if i == max_vocab_size:\n",
    "#             break\n",
    "            \n",
    "#     return embedding_dict, all_train_tokens\n",
    "  \n",
    "# # download the vectors yourself\n",
    "# fasttext_embedding_dict, all_fasttext_tokens = load_vectors('wiki-news-300d-1M.vec')\n",
    "  \n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, list_of_token_lists):\n",
    "        self.input_tensors = []\n",
    "        self.target_tensors = []\n",
    "\n",
    "        for sample in list_of_token_lists:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
    "\n",
    "\n",
    "def tokenize_dataset(datasets, dictionary):\n",
    "    tokenized_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        _current_dictified = []\n",
    "        for l in tqdm(dataset):\n",
    "            l = ['<bos>'] + l + ['<eos>']\n",
    "            encoded_l = dictionary.encode_token_seq(l)\n",
    "            _current_dictified.append(encoded_l)\n",
    "        tokenized_datasets[split] = _current_dictified\n",
    "    return tokenized_datasets\n",
    "\n",
    "def tokenize_mnli_dataset(datasets, dictionary):\n",
    "    tokenized_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        _current_dictified = []\n",
    "        for s1, s2 in tqdm(dataset):\n",
    "            s1 = ['<bos>'] + s1 + ['<eos>']\n",
    "            s2 = ['<bos>'] + s2 + ['<eos>']\n",
    "            encoded_s1 = dictionary.encode_token_seq(s1)            \n",
    "            encoded_s2 = dictionary.encode_token_seq(s2)\n",
    "            _current_dictified.append([encoded_s1, encoded_s2])\n",
    "        tokenized_datasets[split] = _current_dictified\n",
    "    return tokenized_datasets\n",
    "\n",
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat(\n",
    "            [t, torch.tensor([[pad_token] * (max_length - t.size(-1))], dtype=torch.long)], dim=-1)\n",
    "        padded_list.append(padded_tensor)\n",
    "\n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "def pad_collate_fn(pad_idx, batch):\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_idx)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_idx)\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "\n",
    "def load_wikitext(data_dir):\n",
    "    import subprocess\n",
    "    filename = os.path.join(data_dir, 'wikitext2-sentencized.json')\n",
    "    if not os.path.exists(filename):\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        url = \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\"\n",
    "        args = ['wget', '-O', filename, url]\n",
    "        subprocess.call(args)\n",
    "    raw_datasets = json.load(open(filename, 'r'))\n",
    "    for name in raw_datasets:\n",
    "        raw_datasets[name] = [x.split() for x in raw_datasets[name]]\n",
    "\n",
    "    if os.path.exists(os.path.join(data_dir, 'vocab.pkl')):\n",
    "        vocab = pickle.load(open(os.path.join(data_dir, 'vocab.pkl'), 'rb'))\n",
    "    else:\n",
    "        vocab = Dictionary(raw_datasets, include_valid=False)\n",
    "        pickle.dump(vocab, open(os.path.join(data_dir, 'vocab.pkl'), 'wb'))\n",
    "\n",
    "    tokenized_datasets = tokenize_dataset(raw_datasets, vocab)\n",
    "    datasets = {name: LMDataset(ds) for name, ds in tokenized_datasets.items()}\n",
    "    print(\"Vocab size: %d\" % (len(vocab)))\n",
    "    print(\" padding index {}\".format(vocab.get_id('<pad>')))\n",
    "    return raw_datasets, datasets, vocab\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        \n",
    "        # add special tokens\n",
    "        self.add_token('<bos>')\n",
    "        self.add_token('<eos>')\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>')\n",
    "        \n",
    "        for line in tqdm(datasets['train']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "                    \n",
    "        if include_valid is True:\n",
    "            for line in tqdm(datasets['valid']):\n",
    "                for w in line:\n",
    "                    self.add_token(w)\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "9pqDM4ym1WmF"
   },
   "outputs": [],
   "source": [
    "# def perplexity(model, sequences):\n",
    "#     n_total = 0\n",
    "#     logp_total = 0\n",
    "#     for sequence in sequences:\n",
    "#         logp_total += model.sequence_logp(sequence)\n",
    "#         n_total += len(sequence) + 1  \n",
    "#     ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
    "#     return ppl\n",
    "\n",
    "\n",
    "\n",
    "def init_wikitext_dataset(): #same as grade\n",
    "    \"\"\"\n",
    "    Fill in the details\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_datasets, datasets, vocab = load_wikitext(os.getcwd())\n",
    "\n",
    "    data_loaders = {name: DataLoader(datasets[name], batch_size=32, shuffle=True,\n",
    "                                     collate_fn=lambda x: pad_collate_fn(vocab.get_id('<pad>'), x))\n",
    "                    for name in datasets}\n",
    "    \n",
    "    wikitext_val = data_loaders['valid'] \n",
    "    wikitext_train = data_loaders['train'] \n",
    "    wikitext_test = data_loaders['test'] \n",
    "    \n",
    "    #wiki_dict = model_CP.Dictionary(datasets, include_valid=True)\n",
    "    \n",
    "    return wikitext_train, wikitext_val, wikitext_test #, wiki_dict ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 3089,
     "status": "ok",
     "timestamp": 1571688254314,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "c1Aq_lP1zBue",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f3181350-e18a-4298-b4bf-d723c95b7e46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [00:00<00:00, 117288.21it/s]\n",
      "100%|██████████| 8464/8464 [00:00<00:00, 86327.61it/s]\n",
      "100%|██████████| 9708/9708 [00:00<00:00, 48274.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 33178\n",
      " padding index 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2b24d7be6d10>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2b24d7be6810>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2b24d7be6e90>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_wikitext_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "aYZmD7pNgj0C"
   },
   "source": [
    "#### Part B   \n",
    "Here we design our own feature extractor. In MNIST that was a resnet because we were dealing with images. Now we need to pick a model that can model sequences better. Design an RNN-based model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "dVFvc2U-gj0C"
   },
   "outputs": [],
   "source": [
    "class LSTM_CP(nn.Module):\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        \n",
    "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        lstm_outputs = self.lstm(embeddings)\n",
    "        logits = self.projection(lstm_outputs[0])\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        rnn_outputs = self.rnn(embeddings)\n",
    "        logits = self.projection(rnn_outputs[0])\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def init_feature_extractor(): \n",
    "    num_embeddings = 33178#len(vocab)\n",
    "    embedding_size = 128\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    rnn_dropout = 0.1\n",
    "\n",
    "    options = {\n",
    "        'num_embeddings': num_embeddings, #len(wiki_dict),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'padding_idx': 2,\n",
    "        'input_size': embedding_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'rnn_dropout': rnn_dropout,\n",
    "    }\n",
    "    \n",
    "    #feature_extractor =  RNNLanguageModel(options)\n",
    "    feature_extractor =  LSTM_CP(options)\n",
    "    #feature_extractor.projection = Identity() #we remove the last layer for now\n",
    "    \n",
    "    return feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "v3R5i2HLgj0E"
   },
   "source": [
    "#### Part C\n",
    "Pretrain the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "bzUbb9HNgj0F"
   },
   "outputs": [],
   "source": [
    "def fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val):\n",
    "    # FILL IN THE DETAILS\n",
    "    #define current_device\n",
    "    model = feature_extractor\n",
    "    \n",
    "    current_device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    #define criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=2, reduction='sum') #2 is <pad>\n",
    "    #no freezing yet, we fit the feature extractor\n",
    "    param_list = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(param_list, lr=2e-05, eps=1e-08)\n",
    "    model.to(current_device)\n",
    "    plot_cache = []\n",
    "\n",
    "    for epoch_number in range(5):\n",
    "        avg_loss=0\n",
    "        model.train()\n",
    "\n",
    "        train_loss_cache = 0\n",
    "        train_non_pad_tokens_cache = 0\n",
    "        for i, (inp, target) in enumerate(wikitext_train):\n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            train_loss_cache += loss.item()  # still sum here\n",
    "\n",
    "          ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
    "            non_pad_tokens = target.view(-1).ne(2).sum().item() #2 is index for <pad>\n",
    "\n",
    "            train_non_pad_tokens_cache += non_pad_tokens\n",
    "\n",
    "            loss /= non_pad_tokens  # very important to normalize your current loss before you run .backward()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                avg_loss = train_loss_cache / train_non_pad_tokens_cache\n",
    "                avg_ppl = 2**(avg_loss/numpy.log(2))\n",
    "                print('Epoch {} Step {} avg train loss = {:.{prec}f} perplexity = {:.{prec}f}'.format(epoch_number, i, avg_loss, avg_ppl, prec=4))\n",
    "                #train_log_cache = []\n",
    "\n",
    "      #do valid\n",
    "        avg_val_loss = 0\n",
    "        avg_val_ppl = 0\n",
    "        valid_loss_cache = 0\n",
    "        valid_non_pad_tokens_cache = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inp, target) in enumerate(wikitext_val):\n",
    "                inp = inp.to(current_device)\n",
    "                target = target.to(current_device)\n",
    "                logits = model(inp)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "                valid_loss_cache += loss.item()  # still sum here\n",
    "\n",
    "              ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
    "                non_pad_tokens = target.view(-1).ne(2).sum().item()# 2 is index for <pad>\n",
    "\n",
    "                valid_non_pad_tokens_cache += non_pad_tokens\n",
    "\n",
    "            avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n",
    "            avg_val_ppl = 2**(avg_val_loss/numpy.log(2))\n",
    "\n",
    "            print('Validation loss after {} epoch = {:.{prec}f} perplexity = {:.{prec}f}'.format(epoch_number, avg_val_loss, avg_val_ppl,prec=4))\n",
    "\n",
    "        plot_cache.append((avg_loss, avg_val_loss))\n",
    "        #save model\n",
    "\n",
    "    PATH_TO_FOLDER=  '/scratch/cp2530/myjupyter/'\n",
    "    torch.save({\n",
    "            'epoch': 5,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_loss': avg_loss,\n",
    "            'train_perplexity': avg_ppl,\n",
    "            'val_loss':avg_val_loss,\n",
    "            'val_perplexity':avg_val_ppl, \n",
    "            'plot_cache': plot_cache\n",
    "           \n",
    "            }, PATH_TO_FOLDER + \"models/LSTMfeatextract_CP_2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "_vPhO-bCgj0H"
   },
   "source": [
    "#### Part D\n",
    "Calculate the test perplexity on wikitext2. Feel free to recycle code from previous assignments from this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "mawZmdK1gj0I"
   },
   "outputs": [],
   "source": [
    "def calculate_wiki2_test_perplexity(feature_extractor, wikitext_test):\n",
    "    model = feature_extractor\n",
    "    current_device = torch.device(\"cuda:0\")\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=2, reduction='sum')\n",
    "    \n",
    "    valid_loss_cache = 0\n",
    "    valid_non_pad_tokens_cache = 0\n",
    "    avg_val_loss = 0\n",
    "    avg_val_ppl = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, target) in enumerate(wikitext_test):\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            valid_loss_cache += loss.item()  # still sum here\n",
    "\n",
    "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
    "            non_pad_tokens = target.view(-1).ne(2).sum().item()# 2 is index for <pad>\n",
    "\n",
    "            valid_non_pad_tokens_cache += non_pad_tokens\n",
    "\n",
    "        avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n",
    "        avg_val_ppl = 2**(avg_val_loss/numpy.log(2))\n",
    "    \n",
    "    \n",
    "    test_ppl = avg_val_ppl\n",
    "    \n",
    "    return test_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lYrte46Egj0K"
   },
   "source": [
    "#### Let's grade your results!\n",
    "(don't touch this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 1158977,
     "status": "error",
     "timestamp": 1571691231083,
     "user": {
      "displayName": "CP Pan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCDzV3RY3OMK1Rs00QaE_OQtQM27_DVt0PfsL2GFA=s64",
      "userId": "06123485655127339690"
     },
     "user_tz": 240
    },
    "id": "oO1kvNzsgj0L",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "97062c21-b6ac-4a2c-ec29-95e301c89cbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [00:00<00:00, 89388.07it/s]\n",
      "100%|██████████| 8464/8464 [00:00<00:00, 110954.25it/s]\n",
      "100%|██████████| 9708/9708 [00:00<00:00, 90987.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 33178\n",
      " padding index 2\n",
      "Epoch 0 Step 0 avg train loss = 10.4174 perplexity = 33435.7746\n",
      "Epoch 0 Step 500 avg train loss = 8.9709 perplexity = 7870.8750\n",
      "Epoch 0 Step 1000 avg train loss = 8.0873 perplexity = 3252.9791\n",
      "Epoch 0 Step 1500 avg train loss = 7.7476 perplexity = 2316.0394\n",
      "Epoch 0 Step 2000 avg train loss = 7.5710 perplexity = 1941.0217\n",
      "Validation loss after 0 epoch = 6.8274 perplexity = 922.7822\n",
      "Epoch 1 Step 0 avg train loss = 6.7899 perplexity = 888.8265\n",
      "Epoch 1 Step 500 avg train loss = 6.9786 perplexity = 1073.3803\n",
      "Epoch 1 Step 1000 avg train loss = 6.9722 perplexity = 1066.5397\n",
      "Epoch 1 Step 1500 avg train loss = 6.9646 perplexity = 1058.4797\n",
      "Epoch 1 Step 2000 avg train loss = 6.9575 perplexity = 1050.9641\n",
      "Validation loss after 1 epoch = 6.7250 perplexity = 832.9360\n",
      "Epoch 2 Step 0 avg train loss = 6.9859 perplexity = 1081.3285\n",
      "Epoch 2 Step 500 avg train loss = 6.8691 perplexity = 962.0473\n",
      "Epoch 2 Step 1000 avg train loss = 6.8556 perplexity = 949.1375\n",
      "Epoch 2 Step 1500 avg train loss = 6.8421 perplexity = 936.4771\n",
      "Epoch 2 Step 2000 avg train loss = 6.8318 perplexity = 926.8651\n",
      "Validation loss after 2 epoch = 6.6027 perplexity = 737.0494\n",
      "Epoch 3 Step 0 avg train loss = 6.8803 perplexity = 972.9025\n",
      "Epoch 3 Step 500 avg train loss = 6.7656 perplexity = 867.4439\n",
      "Epoch 3 Step 1000 avg train loss = 6.7574 perplexity = 860.4204\n",
      "Epoch 3 Step 1500 avg train loss = 6.7467 perplexity = 851.2692\n",
      "Epoch 3 Step 2000 avg train loss = 6.7343 perplexity = 840.7954\n",
      "Validation loss after 3 epoch = 6.4956 perplexity = 662.2341\n",
      "Epoch 4 Step 0 avg train loss = 6.6213 perplexity = 750.8980\n",
      "Epoch 4 Step 500 avg train loss = 6.6610 perplexity = 781.3241\n",
      "Epoch 4 Step 1000 avg train loss = 6.6507 perplexity = 773.3293\n",
      "Epoch 4 Step 1500 avg train loss = 6.6398 perplexity = 764.9047\n",
      "Epoch 4 Step 2000 avg train loss = 6.6307 perplexity = 757.9895\n",
      "Validation loss after 4 epoch = 6.4058 perplexity = 605.3653\n",
      "test_ppl : 578.2358449114349\n"
     ]
    }
   ],
   "source": [
    "def grade_wikitext2():\n",
    "    # load data\n",
    "    wikitext_train, wikitext_val, wikitext_test = init_wikitext_dataset()\n",
    "\n",
    "    # load feature extractor\n",
    "    feature_extractor = init_feature_extractor()\n",
    "\n",
    "    # pretrain using the feature extractor\n",
    "    fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val)\n",
    "\n",
    "    # check test accuracy\n",
    "    test_ppl = calculate_wiki2_test_perplexity(feature_extractor, wikitext_test)\n",
    "    print(\"test_ppl : {}\".format( test_ppl)) #should be <200\n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_ppl < 10000, 'ummm... your perplexity is too high...'\n",
    "    \n",
    "grade_wikitext2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "LyXHUgfwgj0N"
   },
   "source": [
    "---   \n",
    "## Question 3 (fine-tune on MNLI) REMOVED\n",
    "In this question you will use your feature_extractor from question 2\n",
    "to fine-tune on MNLI.\n",
    "\n",
    "(From the website):\n",
    "The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.\n",
    "\n",
    "MNLI has 3 genres (3 classes).\n",
    "The goal of this question is to maximize the test accuracy in MNLI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ETT_D3N8gj0O"
   },
   "source": [
    "### Part A\n",
    "In this section you need to generate the training, validation and test split. Feel free to use code from your previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "OD3f6OIbgj0P",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import MultiNLI\n",
    "\n",
    "def init_mnli_dataset():\n",
    "    \"\"\"\n",
    "    Fill in the details\n",
    "    \"\"\"\n",
    "    mnli_val = None # TODO\n",
    "    mnli_train = None # TODO\n",
    "    mnli_test = None # TODO\n",
    "    \n",
    "    return mnli_train, mnli_val, mnli_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "zTqFViQzgj0T"
   },
   "source": [
    "### Part B\n",
    "Here we again design a model for finetuning. Use the output of your feature-extractor as the input to this model. This should be a powerful classifier (up to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "ef3tmPHvgj0U",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def init_finetune_model():\n",
    "    \n",
    "    # TODO FILL IN THE DETAILS\n",
    "    fine_tune_model = ...\n",
    "    \n",
    "    return fine_tune_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "smeY4qQbgj0W"
   },
   "source": [
    "### Part C\n",
    "Use the feature_extractor and your fine_tune_model to fine_tune MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "0tl9ktqAgj0W",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fine_tune_mnli(feature_extractor, fine_tune_model, mnli_train, mnli_val):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "0KFgMcsWgj0Y"
   },
   "source": [
    "### Part D\n",
    "Evaluate the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "5b1v5SbGgj0Y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mnli_test_accuracy(feature_extractor, fine_tune_model, mnli_test):\n",
    "    \n",
    "    # YOUR CODE HERE...\n",
    "    \n",
    "    return test_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Tia6Q-mlgj0b"
   },
   "source": [
    "### Let's grade your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "8A8b1-Nrgj0c",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def grade_mnli():\n",
    "    # load data\n",
    "    mnli_train, mnli_val, mnli_test = init_mnli_dataset()\n",
    "\n",
    "    # no need to load feature extractor because it is fine-tuned\n",
    "    feature_extractor = feature_extractor\n",
    "\n",
    "    # init the fine_tune model\n",
    "    fine_tune_model = init_finetune_model()\n",
    "    \n",
    "    # finetune\n",
    "    fine_tune_mnli(feature_extractor, fine_tune_model, mnli_train, mnli_val)\n",
    "\n",
    "    # check test accuracy\n",
    "    test_accuracy = calculate_mnli_test_accuracy(feature_extractor, wikitext_test)\n",
    "\n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_accuracy > 0.00, 'ummm... your accuracy is too low...'\n",
    "    \n",
    "grade_mnli()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ppI0bUQlgj0e"
   },
   "source": [
    "---  \n",
    "### Question 4 (BERT)\n",
    "\n",
    "A major direction in research came from a model called BERT, released last year.  \n",
    "\n",
    "In this question you'll use BERT as your feature_extractor instead of the model you\n",
    "designed yourself.\n",
    "\n",
    "To get BERT, head on over to (https://github.com/huggingface/transformers) and load your BERT model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "GRCrMCHrgj0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 17.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: numpy in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from transformers) (1.17.2)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
      "\u001b[K     |████████████████████████████████| 860kB 25.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from transformers) (4.36.1)\n",
      "Collecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/41/27fb3969a76240d4c42a8f64b9d5ae78c676bab38e980e03b1bbaef279bd/boto3-1.10.2-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 34.9MB/s eta 0:00:01    |███████████████████████         | 92kB 50.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
      "\u001b[K     |████████████████████████████████| 655kB 56.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/cf/7089b87fdae8f47be81ce8e2e6377b321805c4648f2eb12fbd2987388dac/sentencepiece-0.1.83-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 55.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from requests->transformers) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: six in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Collecting click\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 64.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.14.0,>=1.13.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/57/6cd269b6c243f5409fd60b480bf7b9c98a10fa9bb083af223189d7617db5/botocore-1.13.2-py2.py3-none-any.whl (5.3MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3MB 64.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.3.0,>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "\u001b[K     |████████████████████████████████| 552kB 44.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.2->boto3->transformers) (2.8.0)\n",
      "Building wheels for collected packages: sacremoses, regex\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp37-none-any.whl size=883999 sha256=839c56ab4980d133543a669727562e3acd13d456d7d695621373f145c2d0c006\n",
      "  Stored in directory: /home/cp2530/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2019.8.19-cp37-cp37m-linux_x86_64.whl size=627284 sha256=73787160275dcb5c9840e4af4f3b6aa8301bfbed10203c4c338a0a838ba70120\n",
      "  Stored in directory: /home/cp2530/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
      "Successfully built sacremoses regex\n",
      "Installing collected packages: click, joblib, sacremoses, jmespath, docutils, botocore, s3transfer, boto3, regex, sentencepiece, transformers\n",
      "Successfully installed boto3-1.10.2 botocore-1.13.2 click-7.0 docutils-0.15.2 jmespath-0.9.4 joblib-0.14.0 regex-2019.8.19 s3transfer-0.2.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/9d/217fc3c4fe19123fcb99385a35c3211e65d5eb07fbe8dd0008fae0d1fe74/pandas-0.25.2-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4MB 17.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 61.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from pandas) (1.17.2)\n",
      "Requirement already satisfied: six>=1.5 in /scratch/cp2530/miniconda2/envs/NLPClass/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-0.25.2 pytz-2019.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from will\n",
    "from transformers.data.processors.glue import MnliProcessor\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import tempfile\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "\n",
    "\n",
    "# from transformers import (\n",
    "#     BertModel,\n",
    "#     BertTokenizer\n",
    "# )\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from will\n",
    "TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"SNLI\", \"QNLI\", \"RTE\", \"WNLI\", \"diagnostic\"]\n",
    "TASK2PATH = {\n",
    "    \"CoLA\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\",  # noqa\n",
    "    \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",  # noqa\n",
    "    \"MRPC\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\",  # noqa\n",
    "    \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP-clean.zip?alt=media&token=11a647cb-ecd3-49c9-9d31-79f8ca8fe277\",  # noqa\n",
    "    \"STS\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\",  # noqa\n",
    "    \"MNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\",  # noqa\n",
    "    \"SNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\",  # noqa\n",
    "    \"QNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\",  # noqa\n",
    "    \"RTE\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\",  # noqa\n",
    "    \"WNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\",  # noqa\n",
    "    \"diagnostic\": [\n",
    "        \"https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D\",  # noqa\n",
    "        \"https://www.dropbox.com/s/ju7d95ifb072q9f/diagnostic-full.tsv?dl=1\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n",
    "MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n",
    "\n",
    "\n",
    "def download_and_extract(task, data_dir):\n",
    "    print(\"Downloading and extracting %s...\" % task)\n",
    "    data_file = \"%s.zip\" % task\n",
    "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
    "    with zipfile.ZipFile(data_file) as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(data_file)\n",
    "    print(\"\\tCompleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting MNLI...\n",
      "\tCompleted!\n"
     ]
    }
   ],
   "source": [
    "download_and_extract('MNLI', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MnliProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnli_bert_dataloaders():\n",
    "  # ----------------------\n",
    "  # TRAIN/VAL DATALOADERS\n",
    "  # ----------------------\n",
    "  train = processor.get_train_examples('MNLI')\n",
    "  features = convert_examples_to_features(train,\n",
    "                                          tokenizer,\n",
    "                                          label_list=['contradiction','neutral','entailment'],\n",
    "                                          max_length=128,\n",
    "                                          output_mode='classification',\n",
    "                                          pad_on_left=False,\n",
    "                                          pad_token=tokenizer.pad_token_id,\n",
    "                                          pad_token_segment_id=0)\n",
    "  train_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.label for f in features], dtype=torch.long))\n",
    "\n",
    "  nb_train_samples = int(0.95 * len(train_dataset))\n",
    "  nb_val_samples = len(train_dataset) - nb_train_samples\n",
    "\n",
    "  bert_mnli_train_dataset, bert_mnli_val_dataset = random_split(train_dataset, [nb_train_samples, nb_val_samples])\n",
    "\n",
    "  # train loader\n",
    "  train_sampler = RandomSampler(bert_mnli_train_dataset)\n",
    "  bert_mnli_train_dataloader = DataLoader(bert_mnli_train_dataset, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "  # val loader\n",
    "  val_sampler = RandomSampler(bert_mnli_val_dataset)\n",
    "  bert_mnli_val_dataloader = DataLoader(bert_mnli_val_dataset, sampler=val_sampler, batch_size=32)\n",
    "\n",
    "\n",
    "  # ----------------------\n",
    "  # TEST DATALOADERS\n",
    "  # ----------------------\n",
    "  dev = processor.get_dev_examples('MNLI')\n",
    "  features = convert_examples_to_features(dev,\n",
    "                                          tokenizer,\n",
    "                                          label_list=['contradiction','neutral','entailment'],\n",
    "                                          max_length=128,\n",
    "                                          output_mode='classification',\n",
    "                                          pad_on_left=False,\n",
    "                                          pad_token=tokenizer.pad_token_id,\n",
    "                                          pad_token_segment_id=0)\n",
    "\n",
    "  bert_mnli_test_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n",
    "                                torch.tensor([f.label for f in features], dtype=torch.long))\n",
    "\n",
    "  # test dataset\n",
    "  test_sampler = RandomSampler(bert_mnli_test_dataset)\n",
    "  bert_mnli_test_dataloader = DataLoader(bert_mnli_test_dataset, sampler=test_sampler, batch_size=32)\n",
    "  \n",
    "  return bert_mnli_train_dataloader, bert_mnli_val_dataloader, bert_mnli_test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kV_h25wjgj0g"
   },
   "source": [
    "### Part A (init BERT)\n",
    "In this section you need to create an instance of BERT and return if from the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "ZsPbwAvZgj0h"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "def init_bert():\n",
    "    BERT=BertModel.from_pretrained('bert-base-cased', output_attentions=True) # ... YOUR CODE HERE\n",
    "    \n",
    "    return BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_feature_extractor = init_bert()\n",
    "#num_ftrs = BERT_feature_extractor.BertPooler.in_features\n",
    "#BERT_feature_extractor #check if fc layer exist\n",
    "#num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_feature_extractor.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "HhL-xJVzgj0j"
   },
   "source": [
    "## Part B (fine-tune with BERT)\n",
    "\n",
    "Use BERT as your feature extractor to finetune MNLI. Use a new finetune model (reset weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_MNLIClassifier(nn.Module):\n",
    "    def __init__(self, bert, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(bert.config.hidden_size, 100),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(100, num_classes))\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        h, _, attn = self.bert(input_ids=input_ids, \n",
    "                               attention_mask=attention_mask, \n",
    "                               token_type_ids=token_type_ids)\n",
    "        h_cls = h[:, 0]\n",
    "        logits = self.fc(h_cls)\n",
    "        return logits, attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init_finetune_model(bert, num_classes):\n",
    "    model = BERT_MNLIClassifier (bert, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "qS66P1fmgj0j",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fine_tune_mnli_BERT(BERT_feature_extractor, fine_tune_model, mnli_train, mnli_val):\n",
    "    \n",
    "    \n",
    "    #####need to fix this####\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    #num_epochs = 3\n",
    "    # INSERT YOUR CODE: (train the fine_tune model using features extracted by feature_extractor)\n",
    "    #first freeze the layers\n",
    "    freeze_model(feature_extractor)\n",
    "    \n",
    "    #create the finetune model\n",
    "    feature_extractor.fc = fine_tune_model #this is MLP toplayer\n",
    "    model = feature_extractor\n",
    "    \n",
    "    #create loss etc. \n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accs= []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs): \n",
    "      #train\n",
    "       \n",
    "      for i , (images, labels) in enumerate(mnist_train):\n",
    "            images = np.repeat(images, 3, axis=1) #convert to 3 channel\n",
    "            inputs, labels = images.to(device), labels.to(device)\n",
    "            #print(inputs.shape)\n",
    "            #inputs = inputs.unsqueeze_(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 500== 0:\n",
    "                val_acc = test_model(mnist_val, model)\n",
    "                val_accs.append(val_accs)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train Loss {}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(mnist_train), loss,  val_acc))\n",
    "                model.train() #go back to training\n",
    "  \n",
    "    return model, train_losses, val_accs\n",
    "    \n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "mOQJ_9epgj0l"
   },
   "source": [
    "## Part C\n",
    "Evaluate how well we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Y1PWlx9ygj0l",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mnli_test_accuracy_BERT(feature_extractor, fine_tune_model, mnli_test):\n",
    "    \n",
    "    # YOUR CODE HERE...\n",
    "    \n",
    "    return test_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "U0hloSoBgj0o"
   },
   "source": [
    "## Let's grade your BERT results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "PEPGUu8vgj0o",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def grade_mnli_BERT():\n",
    "    BERT_feature_extractor = init_bert()\n",
    "    num_ftrs_Bert = 768 #from printing BERT_feature_extractor\n",
    "    \n",
    "    # load data\n",
    "    mnli_train, mnli_val, mnli_test = generate_mnli_bert_dataloaders()#init_mnli_dataset()\n",
    "\n",
    "    # init the fine_tune model\n",
    "    fine_tune_model = init_finetune_model(num_ftrs_Bert)\n",
    "    \n",
    "    # finetune\n",
    "    fine_tune_mnli(BERT_feature_extractor, fine_tune_model, mnli_train, mnli_val)\n",
    "\n",
    "    # check test accuracy\n",
    "    test_accuracy = calculate_mnli_test_accuracy(feature_extractor, wikitext_test)\n",
    "    \n",
    "    # the real threshold will be released by Oct 11 \n",
    "    assert test_ppl > 0.0, 'ummm... your accuracy is too low...'\n",
    "    \n",
    "grade_mnli_BERT()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
